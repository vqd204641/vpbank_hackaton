import pandas as pd
import numpy as np
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from joblib import dump, load
import os

# H√†m hu·∫•n luy·ªán v√† l∆∞u m√¥ h√¨nh
def train_and_save_model(data_path, output_dir="./classify_model", optimal_k=4):
    os.makedirs(output_dir, exist_ok=True)

    # ƒê·ªçc d·ªØ li·ªáu
    data = pd.read_csv(data_path)
    core_jars = ["NEC", "FFA", "EDU", "LTSS", "PLAY"]
    data_core = data[data['jar'].isin(core_jars)]

    # T·∫°o b·∫£ng pivot: trung b√¨nh t·ª∑ tr·ªçng qua c√°c th√°ng
    pivot_percent = data_core.pivot_table(
        values='percent', index='user_id', columns='jar', aggfunc='mean'
    ).fillna(0)
    for jar in core_jars:
        if jar not in pivot_percent.columns:
            pivot_percent[jar] = 0

    # T√≠nh trung b√¨nh income
    pivot_income = data_core.groupby('user_id')['income'].mean().reset_index()

    # Ph√¢n kho·∫£ng income
    bins = np.arange(0, pivot_income['income'].max() + 5_000_000, 5_000_000)
    labels = [f'{int(i/1e6)}-{int((i+5_000_000)/1e6)}M' for i in bins[:-1]]
    pivot_income['income_bin'] = pd.cut(pivot_income['income'], bins=bins, labels=labels, include_lowest=True)
    pivot_income['income_bin_encoded'] = pd.Categorical(pivot_income['income_bin']).codes

    # K·∫øt h·ª£p d·ªØ li·ªáu
    pivot_data = pivot_percent[core_jars].merge(pivot_income[['user_id', 'income_bin_encoded']], on='user_id')
    features = pivot_data[core_jars + ['income_bin_encoded']]

    # Chu·∫©n h√≥a v√† hu·∫•n luy·ªán m√¥ h√¨nh
    scaler = StandardScaler()
    scaled_data = scaler.fit_transform(features)
    kmeans = KMeans(n_clusters=optimal_k, random_state=42)
    kmeans.fit(scaled_data)

    # G√°n nh√£n c·ª•m v√†o d·ªØ li·ªáu
    pivot_data['cluster'] = kmeans.labels_

    print("\nüìä S·ªë l∆∞·ª£ng ng∆∞·ªùi d√πng theo c·ª•m:")
    print(pivot_data['cluster'].value_counts().sort_index())

    cluster_summary = pivot_data.groupby('cluster')[core_jars + ['income_bin_encoded']].mean()
    print("\nüìà Trung b√¨nh c√°c ƒë·∫∑c tr∆∞ng trong m·ªói c·ª•m:")
    print(cluster_summary)

    # L∆∞u user_id, cluster v√† income_bin v√†o file CSV
    cluster_output_path = "user_clusters.csv"

    # K·∫øt h·ª£p income_bin t·ª´ pivot_income ƒë·ªÉ l∆∞u ƒë√∫ng
    full_user_info = pivot_data.merge(
        pivot_income[['user_id', 'income_bin']],
        on='user_id',
        how='left'
    )[['user_id', 'cluster', 'income_bin']]

    full_user_info.to_csv(cluster_output_path, index=False)
    print(f"\nüìÅ ƒê√£ l∆∞u th√¥ng tin ph√¢n c·ª•m ng∆∞·ªùi d√πng v√†o: {cluster_output_path}")

    # L∆∞u m√¥ h√¨nh v√† scaler
    dump(scaler, f"{output_dir}/scaler.joblib")
    dump(kmeans, f"{output_dir}/kmeans.joblib")
    print(f"\n‚úÖ M√¥ h√¨nh v√† scaler ƒë√£ ƒë∆∞·ª£c l∆∞u ·ªü {output_dir}.")

# H√†m ph√¢n lo·∫°i ng∆∞·ªùi d√πng m·ªõi
def classify_new_user(percent_dict=None, income=None, core_jars=["NEC", "FFA", "EDU", "LTSS", "PLAY"]):
    """
    Ph√¢n lo·∫°i ng∆∞·ªùi d√πng m·ªõi d·ª±a tr√™n thu nh·∫≠p, b·ªè qua ph√¢n b·ªï h≈© n·∫øu kh√¥ng c√≥ percent_dict.
    Income l√† b·∫Øt bu·ªôc.

    Args:
        percent_dict (dict, optional): T·ªâ l·ªá ph√¢n b·ªï h≈©, v√≠ d·ª• {"NEC": 55, "FFA": 10, ...}.
        income (float/int): Thu nh·∫≠p c·ªßa ng∆∞·ªùi d√πng (VND), b·∫Øt bu·ªôc.
        core_jars (list): Danh s√°ch h≈© t√†i ch√≠nh, m·∫∑c ƒë·ªãnh ["NEC", "FFA", "EDU", "LTSS", "PLAY"].

    Returns:
        tuple: (cluster_label, income_bin)
            - cluster_label (int): Nh√£n c·ª•m c·ªßa ng∆∞·ªùi d√πng.
            - income_bin (str): Kho·∫£ng thu nh·∫≠p (v√≠ d·ª•: "15-20M").

    Raises:
        ValueError: N·∫øu income kh√¥ng ƒë∆∞·ª£c cung c·∫•p.
    """
    # Ki·ªÉm tra income b·∫Øt bu·ªôc
    if income is None:
        raise ValueError("Thu nh·∫≠p (income) l√† b·∫Øt bu·ªôc.")

    # T·∫£i scaler v√† m√¥ h√¨nh
    base_path = os.path.join(os.path.dirname(__file__), "classify_model")
    scaler_path = os.path.join(base_path, "scaler.joblib")
    kmeans_path = os.path.join(base_path, "kmeans.joblib")

    try:
        scaler = load(scaler_path)
        kmeans = load(kmeans_path)
    except FileNotFoundError:
        raise FileNotFoundError("Kh√¥ng t√¨m th·∫•y scaler.joblib ho·∫∑c kmeans.joblib trong th∆∞ m·ª•c classify_model.")

    # Ph√¢n kho·∫£ng income
    bins = np.arange(0, int(income) + 5_000_000, 5_000_000)
    labels = [f'{int(i/1e6)}-{int((i+5_000_000)/1e6)}M' for i in bins[:-1]]
    try:
        income_bin = pd.cut([income], bins=bins, labels=labels, include_lowest=True)[0]
        income_bin_encoded = pd.Categorical([income_bin]).codes[0]
    except ValueError:
        income_bin = "0-5M"
        income_bin_encoded = 0

    # N·∫øu kh√¥ng c√≥ percent_dict, ch·ªâ s·ª≠ d·ª•ng income_bin_encoded
    if not percent_dict:
        user_features = np.array([[0] * len(core_jars) + [income_bin_encoded]])
    else:
        # T·∫°o DataFrame t·ª´ percent_dict
        user_data = pd.DataFrame([percent_dict])
        user_data = user_data.reindex(columns=core_jars, fill_value=0)
        # K·∫øt h·ª£p v·ªõi income_bin_encoded
        user_features = np.column_stack((user_data[core_jars].values, [income_bin_encoded]))

    # Chu·∫©n h√≥a v√† d·ª± ƒëo√°n
    try:
        user_features_scaled = scaler.transform(user_features)
        cluster_label = kmeans.predict(user_features_scaled)[0]
    except Exception as e:
        raise ValueError(f"L·ªói khi d·ª± ƒëo√°n c·ª•m: {str(e)}")
    
    return cluster_label, income_bin

# Ch·∫°y hu·∫•n luy·ªán n·∫øu script ƒë∆∞·ª£c g·ªçi tr·ª±c ti·∫øp
# if __name__ == "__main__":
    #train_and_save_model("../user_data_ver1/jars_distribution_with_actual.csv")
    #classify_new_user(income= 14000000)